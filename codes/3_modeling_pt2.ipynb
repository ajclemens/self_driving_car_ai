{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Democratizing Autonomous Driving\n",
    "\n",
    "Notebook 3 - Modeling (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Lambda, Conv2D, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import Huber\n",
    "from utils import INPUT_SHAPE, batch_generator\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the big datasets didn't work out too well, we want to test out how would a smaller dataset would perform. We therefore used the Udacity driving simulator to generate our own data. We drove the car for one entire lap and collected the camera images and steering data among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data, labeling columns\n",
    "df = pd.read_csv('../data/data-anthony/driving_log.csv', names=['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace local file paths to cloud file paths\n",
    "df.center = df.center.str.replace('/Users/anthonysmacbook/DSI/week_7/How_to_simulate_a_self_driving_car/data', '../data-anthony')\n",
    "df.left = df.left.str.replace('/Users/anthonysmacbook/DSI/week_7/How_to_simulate_a_self_driving_car/data', '../data-anthony')\n",
    "df.right = df.right.str.replace('/Users/anthonysmacbook/DSI/week_7/How_to_simulate_a_self_driving_car/data', '../data-anthony')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct feature matrix and target vector\n",
    "X = df[['center', 'left', 'right']].values\n",
    "y = df['steering'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although not shown here, the images go through a lot of processing before they are fed into the neural network. The image processing is built into the generator used in the accompanying utils.py file, which all credited to [Naoki Shibuya](https://github.com/naokishibuya/car-behavioral-cloning). This allows us to simultaneously train our model using our GPU while our data is being prepared / images are being processed on the CPU. Additionally, the generator allows us to repeatedly train our model on one lap of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 2636s 132ms/step - loss: 0.0072 - val_loss: 0.0111\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 2631s 132ms/step - loss: 0.0049 - val_loss: 0.0099\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 2602s 130ms/step - loss: 0.0042 - val_loss: 0.0101\n"
     ]
    }
   ],
   "source": [
    "# convolutional neural network\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) # normalize image from -1 to 1\n",
    "model.add(Conv2D(filters=24, kernel_size=(5, 5), activation='elu', strides=2))\n",
    "model.add(Conv2D(filters=36, kernel_size=(5, 5), activation='elu', strides=2))\n",
    "model.add(Conv2D(filters=48, kernel_size=(5, 5), activation='elu', strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='elu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='elu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='elu'))\n",
    "model.add(Dense(50, activation='elu'))\n",
    "model.add(Dense(10, activation='elu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# checkpoints\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.h5',\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='auto')\n",
    "\n",
    "huber = Huber(delta=1.0)\n",
    "\n",
    "# compile model with custom loss function and optimizer\n",
    "model.compile(loss = huber, optimizer = Adam(lr=1.0e-4))\n",
    "\n",
    "# fit with generator\n",
    "# generator allows us to repeatedly train on one lap of data.\n",
    "model_best = model.fit(x=batch_generator('../data/data-anthony', X_train, y_train, 40, True),\n",
    "                        steps_per_epoch=20000,\n",
    "                        epochs=3,\n",
    "                        max_queue_size=1,\n",
    "                        validation_data=batch_generator('../data/data-anthony', X_test, y_test, 40, False),\n",
    "                        validation_steps=len(X_test),\n",
    "                        callbacks=[checkpoint],\n",
    "                        verbose=1)\n",
    "\n",
    "json.dump(model_best, open('../assets/model_best_history', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007211390882730484, 0.00489849504083395, 0.004201926290988922]\n",
      "[0.011095182038843632, 0.009934862144291401, 0.010121087543666363]\n"
     ]
    }
   ],
   "source": [
    "json.dump(model_best, open('model_best_history', 'w'))\n",
    "\n",
    "model_best = json.load(open('model_best_history', 'r'))\n",
    "\n",
    "print(model_best['loss'])\n",
    "print(model_best['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as the best-performing one, except that we are training with 15 epochs in order to explore if it will help generalize to a brand new track that the model has not been exposed to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20000/20000 [==============================] - 2637s 132ms/step - loss: 0.0074 - val_loss: 0.0095\n",
      "Epoch 2/15\n",
      "20000/20000 [==============================] - 2679s 134ms/step - loss: 0.0052 - val_loss: 0.0093\n",
      "Epoch 3/15\n",
      "20000/20000 [==============================] - 2688s 134ms/step - loss: 0.0045 - val_loss: 0.0093\n",
      "Epoch 4/15\n",
      "20000/20000 [==============================] - 2564s 128ms/step - loss: 0.0041 - val_loss: 0.0094\n",
      "Epoch 5/15\n",
      "20000/20000 [==============================] - 2601s 130ms/step - loss: 0.0038 - val_loss: 0.0092\n",
      "Epoch 6/15\n",
      "20000/20000 [==============================] - 2651s 133ms/step - loss: 0.0035 - val_loss: 0.0092\n",
      "Epoch 7/15\n",
      "20000/20000 [==============================] - 2604s 130ms/step - loss: 0.0034 - val_loss: 0.0094\n",
      "Epoch 8/15\n",
      "20000/20000 [==============================] - 2603s 130ms/step - loss: 0.0032 - val_loss: 0.0093\n",
      "Epoch 9/15\n",
      "20000/20000 [==============================] - 2463s 123ms/step - loss: 0.0031 - val_loss: 0.0091\n",
      "Epoch 10/15\n",
      "20000/20000 [==============================] - 2410s 121ms/step - loss: 0.0029 - val_loss: 0.0103\n",
      "Epoch 11/15\n",
      "20000/20000 [==============================] - 2380s 119ms/step - loss: 0.0028 - val_loss: 0.0095\n",
      "Epoch 12/15\n",
      "20000/20000 [==============================] - 2419s 121ms/step - loss: 0.0027 - val_loss: 0.0098\n",
      "Epoch 13/15\n",
      "20000/20000 [==============================] - 2392s 120ms/step - loss: 0.0026 - val_loss: 0.0096\n",
      "Epoch 14/15\n",
      "20000/20000 [==============================] - 2365s 118ms/step - loss: 0.0025 - val_loss: 0.0094\n",
      "Epoch 15/15\n",
      "20000/20000 [==============================] - 2424s 121ms/step - loss: 0.0025 - val_loss: 0.0096\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type History is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-25f7417755d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                         verbose=1)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_15_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_15_epochs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type History is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# convolutional neural network\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) # normalize image from -1 to 1\n",
    "model.add(Conv2D(filters=24, kernel_size=(5, 5), activation='elu', strides=2))\n",
    "model.add(Conv2D(filters=36, kernel_size=(5, 5), activation='elu', strides=2))\n",
    "model.add(Conv2D(filters=48, kernel_size=(5, 5), activation='elu', strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='elu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='elu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='elu'))\n",
    "model.add(Dense(50, activation='elu'))\n",
    "model.add(Dense(10, activation='elu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# checkpoints\n",
    "checkpoint = ModelCheckpoint('../assets/model-{epoch:03d}.h5',\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='auto')\n",
    "\n",
    "huber = Huber(delta=1.0)\n",
    "\n",
    "# compile model with custom loss function and optimizer\n",
    "model.compile(loss = huber, optimizer = Adam(lr=1.0e-4))\n",
    "\n",
    "# fit with generator\n",
    "# generator allows us to repeatedly train on one lap of data.\n",
    "model_15_epochs = model.fit(x=batch_generator('../data/data-anthony', X_train, y_train, 40, True),\n",
    "                        steps_per_epoch=20000,\n",
    "                        epochs=15,\n",
    "                        max_queue_size=1,\n",
    "                        validation_data=batch_generator('../data/data-anthony', X_test, y_test, 40, False),\n",
    "                        validation_steps=len(X_test),\n",
    "                        callbacks=[checkpoint],\n",
    "                        verbose=1)\n",
    "\n",
    "json.dump(model_15_epochs.history, open('../assets/model_15_epochs', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the driving more human-like rather than swerving, we decided to build a custom loss function to penalize the dramatic turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function, inputs y_true and y_pred and outputs loss.\n",
    "# Includes mean squared error plus alpha and beta terms.\n",
    "# alpha penalizes frequent swerves (high frequency swerves)\n",
    "# beta penalizes large steering values (big swerves)\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse = K.sum(K.square(y_true - y_pred)) / K.cast(len(y_true), 'float32')\n",
    "    \n",
    "    sign_change_count = 0\n",
    "    \n",
    "    for i in range(1, len(y_pred)):\n",
    "        if y_pred[i] == 0 or y_pred[i-1] == 0:\n",
    "            continue\n",
    "        elif y_pred[i] / y_pred[i-1] < 0:\n",
    "            sign_change_count += 1\n",
    "                \n",
    "    alpha = 0.1\n",
    "    beta = 0.05\n",
    "    \n",
    "    y_pred_sum = K.sum(K.square(y_pred))\n",
    "    \n",
    "    loss = mse + (K.cast(alpha, 'float32') * K.cast(sign_change_count, 'float32')) + (beta * y_pred_sum)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
